{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMc/kMwGFRYJMo5r/+ewFju"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"unpkMIoTGVbG","executionInfo":{"status":"ok","timestamp":1715153910019,"user_tz":-180,"elapsed":130354,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"}},"outputId":"960981e6-3a5a-4160-ed67-49a4ffd2ab3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu121\n","Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.2.70.tar.gz (46.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n","Building wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.70-cp310-cp310-linux_x86_64.whl size=3589537 sha256=a5e04a39a60010659cf68d3743c848f450d86dd16a406c6278256608c63d0d56\n","  Stored in directory: /root/.cache/pip/wheels/63/2f/93/5376e5a8493eef71ac826935b380b41c5f3391e4efba65c99f\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.70\n"]}],"source":["!pip install llama-cpp-python \\\n","  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121"]},{"cell_type":"code","source":["!wget https://huggingface.co/Elkhayyat17/llama2-Med-gguf/resolve/main/ggml-model-Q5_K_M.gguf\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GFsvs0E3GaaT","executionInfo":{"status":"ok","timestamp":1715153937321,"user_tz":-180,"elapsed":27397,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"}},"outputId":"1af9c80f-c1e5-4aae-dddf-458caa583c33"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-05-08 07:38:30--  https://huggingface.co/Elkhayyat17/llama2-Med-gguf/resolve/main/ggml-model-Q5_K_M.gguf\n","Resolving huggingface.co (huggingface.co)... 18.172.134.124, 18.172.134.24, 18.172.134.88, ...\n","Connecting to huggingface.co (huggingface.co)|18.172.134.124|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs-us-1.huggingface.co/repos/0c/33/0c339bc47ef4bb2188a662e332c337cfed1ab1c16cd0befb7d0c30c2a12fc011/00f7eadaddd044c3530ec0711445a18abdfbede4b877c3301a622f589cadba7b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-model-Q5_K_M.gguf%3B+filename%3D%22ggml-model-Q5_K_M.gguf%22%3B&Expires=1715413110&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNTQxMzExMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBjLzMzLzBjMzM5YmM0N2VmNGJiMjE4OGE2NjJlMzMyYzMzN2NmZWQxYWIxYzE2Y2QwYmVmYjdkMGMzMGMyYTEyZmMwMTEvMDBmN2VhZGFkZGQwNDRjMzUzMGVjMDcxMTQ0NWExOGFiZGZiZWRlNGI4NzdjMzMwMWE2MjJmNTg5Y2FkYmE3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=CFNFCAJJRnOo2Oe5UJr1XG8GNlNfHiPMFR1LQ7BJNKYtzM8QU5g18ClIrWGOB7aeuJzw4rBkK%7EAbLNpBaYS28Zo-%7EV7OaqRCz7n8rfKGy4nNHBbB6f9PLzKgTtCnn-LIj-EzAp-ZYeFi4ZI7ZzN%7EaXJx73CYFeEAirocXbWvYoTJnHxJeGpBzbzPj1mFj8evDWXjgKeq7ntE5xFP0puCgbxodqeY8Z%7EmiRehS8sA5Snp%7EE7JvO0C4oKLOKeiUAD3BEaYVr53d3t%7EHZHf2XMvOX7FfqgPt7gx9PZMzfJlrUYX368Kc0eBXpyB5WJPiXV8WNXfb9zDz30qfi8sASiPGg__&Key-Pair-Id=KCD77M1F0VK2B [following]\n","--2024-05-08 07:38:30--  https://cdn-lfs-us-1.huggingface.co/repos/0c/33/0c339bc47ef4bb2188a662e332c337cfed1ab1c16cd0befb7d0c30c2a12fc011/00f7eadaddd044c3530ec0711445a18abdfbede4b877c3301a622f589cadba7b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-model-Q5_K_M.gguf%3B+filename%3D%22ggml-model-Q5_K_M.gguf%22%3B&Expires=1715413110&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNTQxMzExMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBjLzMzLzBjMzM5YmM0N2VmNGJiMjE4OGE2NjJlMzMyYzMzN2NmZWQxYWIxYzE2Y2QwYmVmYjdkMGMzMGMyYTEyZmMwMTEvMDBmN2VhZGFkZGQwNDRjMzUzMGVjMDcxMTQ0NWExOGFiZGZiZWRlNGI4NzdjMzMwMWE2MjJmNTg5Y2FkYmE3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=CFNFCAJJRnOo2Oe5UJr1XG8GNlNfHiPMFR1LQ7BJNKYtzM8QU5g18ClIrWGOB7aeuJzw4rBkK%7EAbLNpBaYS28Zo-%7EV7OaqRCz7n8rfKGy4nNHBbB6f9PLzKgTtCnn-LIj-EzAp-ZYeFi4ZI7ZzN%7EaXJx73CYFeEAirocXbWvYoTJnHxJeGpBzbzPj1mFj8evDWXjgKeq7ntE5xFP0puCgbxodqeY8Z%7EmiRehS8sA5Snp%7EE7JvO0C4oKLOKeiUAD3BEaYVr53d3t%7EHZHf2XMvOX7FfqgPt7gx9PZMzfJlrUYX368Kc0eBXpyB5WJPiXV8WNXfb9zDz30qfi8sASiPGg__&Key-Pair-Id=KCD77M1F0VK2B\n","Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.156.107.49, 108.156.107.29, 108.156.107.44, ...\n","Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|108.156.107.49|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4783157824 (4.5G) [binary/octet-stream]\n","Saving to: ‘ggml-model-Q5_K_M.gguf’\n","\n","ggml-model-Q5_K_M.g 100%[===================>]   4.45G   172MB/s    in 27s     \n","\n","2024-05-08 07:38:57 (170 MB/s) - ‘ggml-model-Q5_K_M.gguf’ saved [4783157824/4783157824]\n","\n"]}]},{"cell_type":"code","source":["!pip install uvicorn fastapi pyngrok\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xRgMiaa3G4xg","executionInfo":{"status":"ok","timestamp":1715153953884,"user_tz":-180,"elapsed":16666,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"}},"outputId":"1a3f9dfd-aa24-4d00-ab3a-fa79d05775b1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting uvicorn\n","  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi\n","  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyngrok\n","  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.11.0)\n","Collecting starlette<0.38.0,>=0.37.2 (from fastapi)\n","  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.7.1)\n","Collecting fastapi-cli>=0.0.2 (from fastapi)\n","  Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n","Collecting httpx>=0.23.0 (from fastapi)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.1.3)\n","Collecting python-multipart>=0.0.7 (from fastapi)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi)\n","  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting orjson>=3.2.1 (from fastapi)\n","  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi)\n","  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n","Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi)\n","  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi) (3.7)\n","Collecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi)\n","  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (2024.2.2)\n","Collecting httpcore==1.* (from httpx>=0.23.0->fastapi)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi) (2.1.5)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.2)\n","Collecting httptools>=0.5.0 (from uvicorn)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn)\n","  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.4 (from uvicorn)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi) (1.2.1)\n","Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\n","Installing collected packages: websockets, uvloop, ujson, shellingham, python-multipart, python-dotenv, pyngrok, orjson, httptools, h11, dnspython, watchfiles, uvicorn, starlette, httpcore, email_validator, typer, httpx, fastapi-cli, fastapi\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.9.4\n","    Uninstalling typer-0.9.4:\n","      Successfully uninstalled typer-0.9.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.3 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.3 pyngrok-7.1.6 python-dotenv-1.0.1 python-multipart-0.0.9 shellingham-1.5.4 starlette-0.37.2 typer-0.12.3 ujson-5.9.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"]}]},{"cell_type":"code","source":["import io\n","from fastapi import UploadFile, HTTPException, FastAPI\n","from fastapi.middleware.cors import CORSMiddleware\n","from pydantic import BaseModel\n","import torch\n","\n","\n","app = FastAPI()\n","from llama_cpp import Llama\n","llm = Llama(\n","      model_path=\"ggml-model-Q5_K_M.gguf\",\n","       n_gpu_layers=-1,n_ctx=2048,chat_format=\"llama-2\"\n",")\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=[\"*\"],\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],\n","    allow_headers=[\"*\"],\n",")\n","\n","def ask(quest):\n","    out=llm.create_chat_completion(\n","      messages = [{\"role\": \"system\", \"content\": '''You are Doctor Sakena,  a virtual AI doctor known for your friendly and approachable demeanor,\n","  combined with a deep expertise in the medical field. You're here to provide professional, empathetic, and knowledgeable advice on health-related inquiries.\n","  You'll also provide differential diagnosis. If you're unsure about any information, Don't share false information.'''},\n","  {\"role\": \"user\", \"content\": f\" Symptoms:{quest}\"} ],\n","\n","    temperature=0.001,\n",")\n","    return str(out['choices'][0]['message']['content'])\n","\n","\n","\n","\n","class ChatBotRequest(BaseModel):\n","    message: str\n","\n","@app.post(\"/chatBot\")\n","async def detect(request: ChatBotRequest):\n","    return { \"response\":  ask(request.message) }\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mvwPxmQ3GcRq","executionInfo":{"status":"ok","timestamp":1715153979473,"user_tz":-180,"elapsed":25627,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"}},"outputId":"3fd3fcef-7bc8-40aa-bbea-e6d891a1a037"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ggml-model-Q5_K_M.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 17\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n","llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q5_K:  193 tensors\n","llama_model_loader: - type q6_K:   33 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 32\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 4096\n","llm_load_print_meta: n_embd_v_gqa     = 4096\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 11008\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q5_K - Medium\n","llm_load_print_meta: model params     = 6.74 B\n","llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n","llm_load_print_meta: general.name     = LLaMA v2\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.15 MiB\n","llm_load_tensors:        CPU buffer size =  4560.87 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 2048\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n","llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n"]}]},{"cell_type":"code","source":["!ngrok config add-authtoken 2dSYzvI6BTpQoVRCtsb09nxiOEm_5NjwRJ4K2EQxqNykPnr19"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uL-d9JOHH9yD","executionInfo":{"status":"ok","timestamp":1715153980936,"user_tz":-180,"elapsed":1484,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"}},"outputId":"c77d0ba8-dfcb-4251-a5a0-322b14afe8bb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok\n","import nest_asyncio\n","import uvicorn\n","ngrok_tunnel = ngrok.connect(8000)\n","print('Public URL:', ngrok_tunnel.public_url)\n","nest_asyncio.apply()\n","uvicorn.run(app, port=8000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDbNKNiMGd1K","outputId":"9881c245-1f27-4ca7-faa3-9e4e2aaa228f"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Public URL: https://09d9-35-193-149-231.ngrok-free.app\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:     Started server process [563]\n","INFO:     Waiting for application startup.\n","INFO:     Application startup complete.\n","INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:     197.35.54.48:0 - \"GET / HTTP/1.1\" 404 Not Found\n","INFO:     197.35.54.48:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n","INFO:     197.35.54.48:0 - \"GET /docs HTTP/1.1\" 200 OK\n","INFO:     197.35.54.48:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n","INFO:     149.154.161.198:0 - \"GET /docs HTTP/1.1\" 200 OK\n","INFO:     197.35.106.254:0 - \"GET /docs HTTP/1.1\" 200 OK\n","INFO:     197.35.106.254:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"]},{"output_type":"stream","name":"stderr","text":["\n","llama_print_timings:        load time =   62543.85 ms\n","llama_print_timings:      sample time =      29.26 ms /    45 runs   (    0.65 ms per token,  1538.04 tokens per second)\n","llama_print_timings: prompt eval time =   62543.61 ms /   114 tokens (  548.63 ms per token,     1.82 tokens per second)\n","llama_print_timings:        eval time =   33274.52 ms /    44 runs   (  756.24 ms per token,     1.32 tokens per second)\n","llama_print_timings:       total time =   96001.97 ms /   158 tokens\n"]},{"output_type":"stream","name":"stdout","text":["INFO:     197.35.106.254:0 - \"POST /chatBot HTTP/1.1\" 200 OK\n","INFO:     197.35.106.254:0 - \"GET /docs HTTP/1.1\" 200 OK\n","INFO:     197.35.106.254:0 - \"GET / HTTP/1.1\" 404 Not Found\n","INFO:     197.35.106.254:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"plftOO2cIvAz"},"execution_count":null,"outputs":[]}]}